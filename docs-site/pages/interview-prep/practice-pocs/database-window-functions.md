# Window Functions - Stop Using GROUP BY for Everything

**The Problem**: You're writing complex self-joins and subqueries just to calculate rankings, running totals, and comparisons.

**The Solution**: Window functions compute aggregates WITHOUT collapsing rows - keeping all data while adding calculations.

**Time to Implement**: 5 minutes to replace your first GROUP BY nightmare.

---

## üìä The Problem Everyone Faces

You need to show a sales leaderboard with:
- Each salesperson's revenue
- Their rank
- Revenue of the top performer
- Their percentage of top performer's revenue
- Running total

Here's what most developers write:

```sql
SELECT
  s.name,
  s.revenue,
  (
    SELECT COUNT(*) + 1
    FROM salespeople s2
    WHERE s2.revenue > s.revenue
  ) AS rank,
  (SELECT MAX(revenue) FROM salespeople) AS top_revenue,
  ROUND(100.0 * s.revenue / (SELECT MAX(revenue) FROM salespeople), 2) AS pct_of_top,
  (
    SELECT SUM(s3.revenue)
    FROM salespeople s3
    WHERE s3.revenue >= s.revenue
  ) AS running_total
FROM salespeople s
ORDER BY s.revenue DESC;
```

**Problems**:
1. **5 subqueries**: Database scans `salespeople` table 5+ times
2. **Slow**: N¬≤ complexity for ranking calculation
3. **Unreadable**: Nested subqueries everywhere
4. **Unmaintainable**: Adding one more calculation = another subquery

**Performance**:
```
Rows: 1,000
Execution time: 8,500ms
Table scans: 5,000+
```

### The Self-Join Nightmare

You need each user's order count AND average order count:

```sql
-- ‚ùå Complex query with self-join
SELECT
  u.id,
  u.name,
  user_orders.order_count,
  avg_orders.avg_count
FROM users u
LEFT JOIN (
  SELECT customer_id, COUNT(*) AS order_count
  FROM orders
  GROUP BY customer_id
) user_orders ON user_orders.customer_id = u.id
CROSS JOIN (
  SELECT AVG(order_count) AS avg_count
  FROM (
    SELECT COUNT(*) AS order_count
    FROM orders
    GROUP BY customer_id
  ) counts
) avg_orders;
```

**15 lines** of confusing SQL for a simple question.

### The Hidden Cost

At a mid-sized company:
- **Leaderboard queries**: 20+ similar queries
- **Development time**: 1 hour to write each query
- **Debug time**: 30 minutes to fix bugs
- **Performance issues**: 5+ slow queries (>5 seconds each)
- **Database load**: Multiple table scans = high CPU

**Annual cost**: **50 hours** wasted = **$3,750** + database performance degradation.

---

## üí° The Paradigm Shift

**Old Model**: Use GROUP BY to aggregate, losing individual rows.

**New Model**: Use window functions to calculate aggregates while keeping all rows.

**Key Insight**: Window functions add calculated columns **without changing the number of rows**.

Think of window functions like **map() in JavaScript** - transform each row based on a window of rows, without collapsing the dataset.

```
GROUP BY (collapses rows):
100 rows ‚Üí 10 groups ‚Üí 10 rows

Window Functions (keeps rows):
100 rows ‚Üí add calculated column ‚Üí 100 rows
```

---

## ‚úÖ The Solution: Window Functions

A **window function** performs calculations across a set of rows related to the current row - without collapsing rows like GROUP BY.

### Pattern 1: Rankings (Without Subqueries)

```sql
-- ‚úÖ Simple query with window functions
SELECT
  name,
  revenue,
  RANK() OVER (ORDER BY revenue DESC) AS rank,
  MAX(revenue) OVER () AS top_revenue,
  ROUND(100.0 * revenue / MAX(revenue) OVER (), 2) AS pct_of_top,
  SUM(revenue) OVER (ORDER BY revenue DESC) AS running_total
FROM salespeople
ORDER BY revenue DESC;
```

**Result**:
```
name    | revenue | rank | top_revenue | pct_of_top | running_total
--------|---------|------|-------------|------------|---------------
Alice   | 50000   | 1    | 50000       | 100.00     | 50000
Bob     | 45000   | 2    | 50000       | 90.00      | 95000
Charlie | 40000   | 3    | 50000       | 80.00      | 135000
Diana   | 40000   | 3    | 50000       | 80.00      | 175000
```

**Performance**:
```
Rows: 1,000
Execution time: 12ms
Table scans: 1
Improvement: 708x faster
```

**Benefits**:
- ‚úÖ **1 table scan** instead of 5,000+
- ‚úÖ **12ms** instead of 8,500ms
- ‚úÖ **6 lines** instead of 15
- ‚úÖ **Readable** - clear intent

### How Window Functions Work

**Syntax**:
```sql
function_name() OVER (
  [PARTITION BY column]
  [ORDER BY column]
  [ROWS BETWEEN ... AND ...]
)
```

**Components**:
- **function_name()**: Aggregate (SUM, AVG, COUNT) or ranking (ROW_NUMBER, RANK, DENSE_RANK)
- **PARTITION BY**: Divide rows into groups (like GROUP BY, but keeps all rows)
- **ORDER BY**: Order rows within partition
- **ROWS BETWEEN**: Define window frame (sliding window of rows)

### Pattern 2: Ranking Functions

**ROW_NUMBER**: Sequential number (1, 2, 3, 4, ...)
```sql
SELECT
  name,
  revenue,
  ROW_NUMBER() OVER (ORDER BY revenue DESC) AS row_num
FROM salespeople;
```

**Result**:
```
name    | revenue | row_num
--------|---------|--------
Alice   | 50000   | 1
Bob     | 45000   | 2
Charlie | 40000   | 3
Diana   | 40000   | 4  ‚Üê Note: Different from RANK
```

**RANK**: Same rank for ties, skips next rank (1, 2, 3, 3, 5)
```sql
SELECT
  name,
  revenue,
  RANK() OVER (ORDER BY revenue DESC) AS rank
FROM salespeople;
```

**Result**:
```
name    | revenue | rank
--------|---------|-----
Alice   | 50000   | 1
Bob     | 45000   | 2
Charlie | 40000   | 3  ‚Üê Tied
Diana   | 40000   | 3  ‚Üê Tied
Eve     | 35000   | 5  ‚Üê Skipped 4
```

**DENSE_RANK**: Same rank for ties, no gap (1, 2, 3, 3, 4)
```sql
SELECT
  name,
  revenue,
  DENSE_RANK() OVER (ORDER BY revenue DESC) AS dense_rank
FROM salespeople;
```

**Result**:
```
name    | revenue | dense_rank
--------|---------|------------
Alice   | 50000   | 1
Bob     | 45000   | 2
Charlie | 40000   | 3  ‚Üê Tied
Diana   | 40000   | 3  ‚Üê Tied
Eve     | 35000   | 4  ‚Üê No gap
```

**When to use each**:
- **ROW_NUMBER**: Unique row numbers (pagination, deduplication)
- **RANK**: Competitive ranking with gaps (sports leaderboards)
- **DENSE_RANK**: Compact ranking (product ratings, grades)

### Pattern 3: Partition By (Group Calculations Without Grouping)

**Problem**: Show each employee's salary AND department average salary.

```sql
-- ‚ùå Without window functions (self-join)
SELECT
  e.name,
  e.department,
  e.salary,
  dept_avg.avg_salary
FROM employees e
JOIN (
  SELECT department, AVG(salary) AS avg_salary
  FROM employees
  GROUP BY department
) dept_avg ON dept_avg.department = e.department;

-- ‚úÖ With window functions (simple)
SELECT
  name,
  department,
  salary,
  AVG(salary) OVER (PARTITION BY department) AS dept_avg_salary,
  salary - AVG(salary) OVER (PARTITION BY department) AS diff_from_avg
FROM employees;
```

**Result**:
```
name  | department | salary | dept_avg_salary | diff_from_avg
------|------------|--------|-----------------|---------------
Alice | Engineering| 120000 | 105000          | +15000
Bob   | Engineering| 100000 | 105000          | -5000
Carol | Engineering| 95000  | 105000          | -10000
Dave  | Sales      | 80000  | 75000           | +5000
Eve   | Sales      | 70000  | 75000           | -5000
```

**Use cases**:
- Compare individual vs. group average
- Calculate percentage of group total
- Identify outliers (>2 std deviations from mean)

### Pattern 4: Running Totals & Cumulative Calculations

```sql
-- Running total of sales
SELECT
  sale_date,
  daily_revenue,
  SUM(daily_revenue) OVER (ORDER BY sale_date) AS running_total
FROM daily_sales
ORDER BY sale_date;
```

**Result**:
```
sale_date  | daily_revenue | running_total
-----------|---------------|---------------
2024-06-01 | 1000          | 1000
2024-06-02 | 1500          | 2500
2024-06-03 | 1200          | 3700
2024-06-04 | 1800          | 5500
```

**Running average**:
```sql
SELECT
  sale_date,
  daily_revenue,
  AVG(daily_revenue) OVER (ORDER BY sale_date) AS running_avg
FROM daily_sales;
```

**Result**:
```
sale_date  | daily_revenue | running_avg
-----------|---------------|-------------
2024-06-01 | 1000          | 1000.00
2024-06-02 | 1500          | 1250.00  (avg of 1000, 1500)
2024-06-03 | 1200          | 1233.33  (avg of 1000, 1500, 1200)
```

### Pattern 5: Moving Averages (Sliding Window)

```sql
-- 7-day moving average
SELECT
  sale_date,
  daily_revenue,
  AVG(daily_revenue) OVER (
    ORDER BY sale_date
    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
  ) AS moving_avg_7d
FROM daily_sales;
```

**How it works**:
```
Day 1: avg(day1)                                    = 1000
Day 2: avg(day1, day2)                              = 1250
Day 3: avg(day1, day2, day3)                        = 1233
...
Day 7: avg(day1, day2, ..., day7)                   = 1350
Day 8: avg(day2, day3, ..., day8)                   = 1400 ‚Üê Sliding window
Day 9: avg(day3, day4, ..., day9)                   = 1420
```

**Result**:
```
sale_date  | daily_revenue | moving_avg_7d
-----------|---------------|---------------
2024-06-01 | 1000          | 1000.00
2024-06-02 | 1500          | 1250.00
2024-06-03 | 1200          | 1233.33
...
2024-06-08 | 1600          | 1400.00  ‚Üê 7-day window
2024-06-09 | 1550          | 1420.00
```

**Frame options**:
```sql
-- Current row only
ROWS BETWEEN CURRENT ROW AND CURRENT ROW

-- Current + 3 preceding rows
ROWS BETWEEN 3 PRECEDING AND CURRENT ROW

-- Current + 2 preceding + 2 following
ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING

-- All rows from start to current
ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW

-- All rows
ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
```

### Pattern 6: LAG & LEAD (Compare with Previous/Next Row)

**LAG**: Access previous row's value
```sql
SELECT
  sale_date,
  daily_revenue,
  LAG(daily_revenue) OVER (ORDER BY sale_date) AS prev_day_revenue,
  daily_revenue - LAG(daily_revenue) OVER (ORDER BY sale_date) AS change_from_prev
FROM daily_sales;
```

**Result**:
```
sale_date  | daily_revenue | prev_day_revenue | change_from_prev
-----------|---------------|------------------|------------------
2024-06-01 | 1000          | NULL             | NULL
2024-06-02 | 1500          | 1000             | +500
2024-06-03 | 1200          | 1500             | -300
2024-06-04 | 1800          | 1200             | +600
```

**LEAD**: Access next row's value
```sql
SELECT
  sale_date,
  daily_revenue,
  LEAD(daily_revenue) OVER (ORDER BY sale_date) AS next_day_revenue
FROM daily_sales;
```

**Result**:
```
sale_date  | daily_revenue | next_day_revenue
-----------|---------------|------------------
2024-06-01 | 1000          | 1500
2024-06-02 | 1500          | 1200
2024-06-03 | 1200          | 1800
2024-06-04 | 1800          | NULL
```

**Use cases**:
- Day-over-day growth
- Week-over-week comparison
- Churn detection (no activity after this date)
- Gap analysis (missing data detection)

### Pattern 7: FIRST_VALUE & LAST_VALUE

**FIRST_VALUE**: Get first value in window
```sql
SELECT
  name,
  department,
  salary,
  FIRST_VALUE(name) OVER (PARTITION BY department ORDER BY salary DESC) AS highest_paid,
  FIRST_VALUE(salary) OVER (PARTITION BY department ORDER BY salary DESC) AS highest_salary
FROM employees;
```

**Result**:
```
name  | department  | salary | highest_paid | highest_salary
------|-------------|--------|--------------|---------------
Alice | Engineering | 120000 | Alice        | 120000
Bob   | Engineering | 100000 | Alice        | 120000
Carol | Engineering | 95000  | Alice        | 120000
Dave  | Sales       | 80000  | Dave         | 80000
Eve   | Sales       | 70000  | Dave         | 80000
```

**LAST_VALUE**: Get last value in window
```sql
SELECT
  sale_date,
  daily_revenue,
  LAST_VALUE(daily_revenue) OVER (
    ORDER BY sale_date
    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
  ) AS final_revenue
FROM daily_sales;
```

**‚ö†Ô∏è Common mistake**: Forgetting to specify frame
```sql
-- ‚ùå Wrong (default frame is ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)
LAST_VALUE(daily_revenue) OVER (ORDER BY sale_date)
-- Returns current row (not last row)

-- ‚úÖ Correct
LAST_VALUE(daily_revenue) OVER (
  ORDER BY sale_date
  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
)
```

---

## üîß Hands-On: Create Your First Window Function Query (5 Minutes)

Let's build a real-world example: **Product sales dashboard**.

### Step 1: Create Sample Data

```sql
CREATE TABLE product_sales (
  product_id INT,
  product_name VARCHAR(100),
  category VARCHAR(50),
  sale_date DATE,
  quantity INT,
  revenue DECIMAL(10, 2)
);

INSERT INTO product_sales VALUES
  (1, 'Laptop Pro', 'Electronics', '2024-06-01', 10, 15000),
  (1, 'Laptop Pro', 'Electronics', '2024-06-02', 15, 22500),
  (2, 'Mouse', 'Electronics', '2024-06-01', 50, 1000),
  (2, 'Mouse', 'Electronics', '2024-06-02', 60, 1200),
  (3, 'Desk Chair', 'Furniture', '2024-06-01', 8, 2400),
  (3, 'Desk Chair', 'Furniture', '2024-06-02', 12, 3600);
```

### Step 2: Build Dashboard Query

```sql
SELECT
  product_name,
  category,
  sale_date,
  revenue,

  -- Rank within category
  RANK() OVER (PARTITION BY category, sale_date ORDER BY revenue DESC) AS category_rank,

  -- Running total for product
  SUM(revenue) OVER (PARTITION BY product_id ORDER BY sale_date) AS product_running_total,

  -- Day-over-day growth
  revenue - LAG(revenue) OVER (PARTITION BY product_id ORDER BY sale_date) AS daily_change,

  -- Percentage of category total
  ROUND(100.0 * revenue / SUM(revenue) OVER (PARTITION BY category, sale_date), 2) AS pct_of_category
FROM product_sales
ORDER BY category, sale_date, revenue DESC;
```

### Step 3: Analyze Results

```
product_name | category    | sale_date  | revenue | category_rank | product_running_total | daily_change | pct_of_category
-------------|-------------|------------|---------|---------------|----------------------|--------------|----------------
Laptop Pro   | Electronics | 2024-06-01 | 15000   | 1             | 15000                | NULL         | 93.75
Mouse        | Electronics | 2024-06-01 | 1000    | 2             | 1000                 | NULL         | 6.25
Laptop Pro   | Electronics | 2024-06-02 | 22500   | 1             | 37500                | +7500        | 94.94
Mouse        | Electronics | 2024-06-02 | 1200    | 2             | 2200                 | +200         | 5.06
Desk Chair   | Furniture   | 2024-06-01 | 2400    | 1             | 2400                 | NULL         | 100.00
Desk Chair   | Furniture   | 2024-06-02 | 3600    | 1             | 6000                 | +1200        | 100.00
```

**Insights**:
- ‚úÖ Laptop Pro dominates Electronics (93%+ of revenue)
- ‚úÖ All products growing day-over-day
- ‚úÖ Running totals show cumulative performance
- ‚úÖ Rankings updated daily

---

## üè¢ Real-World Examples

### **Google Analytics** (Web Analytics)
- **Use Case**: Bounce rate, session duration percentiles
- **Window Functions**:
  - `PERCENT_RANK()` for percentile calculations
  - `LAG()` for session duration (exit_time - entry_time)
- **Impact**: Real-time dashboard queries <100ms

### **Stripe** (Payment Processing)
- **Use Case**: Customer lifetime value ranking
- **Window Functions**:
  - `SUM() OVER (PARTITION BY customer_id ORDER BY payment_date)` for running LTV
  - `NTILE(100)` for percentile segments (top 1%, top 10%, etc.)
- **Impact**: Merchant dashboard shows rankings without full table scans

### **Shopify** (E-Commerce)
- **Use Case**: Product trending (sales velocity)
- **Window Functions**:
  - `AVG() OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)` for 7-day moving average
  - `LAG()` for week-over-week growth
- **Impact**: Trending products page updates in real-time

### **Uber** (Ride Sharing)
- **Use Case**: Driver earnings leaderboards
- **Window Functions**:
  - `RANK() OVER (PARTITION BY city ORDER BY earnings DESC)` for city-specific rankings
  - `PERCENT_RANK()` for "top 10% of drivers"
- **Impact**: Gamification features (show driver's rank to motivate)

---

## üöÄ Advanced Window Function Patterns

### Pattern 8: NTILE (Buckets/Percentiles)

**Problem**: Divide customers into quartiles (top 25%, next 25%, etc.).

```sql
SELECT
  customer_id,
  total_spent,
  NTILE(4) OVER (ORDER BY total_spent DESC) AS quartile,
  CASE NTILE(4) OVER (ORDER BY total_spent DESC)
    WHEN 1 THEN 'VIP'
    WHEN 2 THEN 'High Value'
    WHEN 3 THEN 'Medium Value'
    ELSE 'Low Value'
  END AS segment
FROM customer_totals;
```

**Result**:
```
customer_id | total_spent | quartile | segment
------------|-------------|----------|-------------
123         | 50000       | 1        | VIP
456         | 40000       | 1        | VIP
789         | 30000       | 2        | High Value
...
```

**Use cases**:
- RFM segmentation (Recency, Frequency, Monetary)
- A/B test group assignment
- Load distribution (split data into N buckets)

### Pattern 9: Gap & Islands (Streak Detection)

**Problem**: Find consecutive days users were active.

```sql
WITH user_activity AS (
  SELECT
    user_id,
    activity_date,
    ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY activity_date) AS rn,
    activity_date - (ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY activity_date))::int AS grp
  FROM user_events
)
SELECT
  user_id,
  MIN(activity_date) AS streak_start,
  MAX(activity_date) AS streak_end,
  COUNT(*) AS streak_length
FROM user_activity
GROUP BY user_id, grp
HAVING COUNT(*) >= 7  -- Streaks of 7+ days
ORDER BY streak_length DESC;
```

**Use cases**:
- Gamification (7-day streak badges)
- Churn detection (gaps > 30 days)
- Attendance tracking
- Uptime monitoring (consecutive hours online)

### Pattern 10: Conditional Aggregation

**Problem**: Count orders only for "completed" status within each customer.

```sql
SELECT
  customer_id,
  order_id,
  status,
  total,
  COUNT(*) FILTER (WHERE status = 'completed') OVER (PARTITION BY customer_id) AS completed_count,
  SUM(total) FILTER (WHERE status = 'completed') OVER (PARTITION BY customer_id) AS completed_revenue
FROM orders;
```

**Result**:
```
customer_id | order_id | status    | total | completed_count | completed_revenue
------------|----------|-----------|-------|-----------------|-------------------
123         | 1        | completed | 100   | 2               | 250
123         | 2        | pending   | 50    | 2               | 250
123         | 3        | completed | 150   | 2               | 250
```

---

## üìà Performance Considerations

### Window Functions are Fast

**Benchmark** (1M rows, PostgreSQL 14):
```sql
-- ‚ùå Subquery approach
SELECT
  name,
  (SELECT AVG(salary) FROM employees e2 WHERE e2.department = e1.department)
FROM employees e1;
-- Execution time: 45 seconds

-- ‚úÖ Window function
SELECT
  name,
  AVG(salary) OVER (PARTITION BY department)
FROM employees;
-- Execution time: 800ms
-- Improvement: 56x faster
```

### Add Indexes for PARTITION BY

```sql
CREATE INDEX idx_employees_department ON employees(department);

SELECT
  name,
  AVG(salary) OVER (PARTITION BY department)
FROM employees;
-- Uses idx_employees_department for efficient partitioning
```

### Limit Window Frame Size

```sql
-- ‚ùå Slow (unbounded window for every row)
SELECT
  sale_date,
  SUM(revenue) OVER (ORDER BY sale_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)
FROM daily_sales;
-- For row 1M, sums 1M rows

-- ‚úÖ Fast (fixed window size)
SELECT
  sale_date,
  SUM(revenue) OVER (ORDER BY sale_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)
FROM daily_sales;
-- For row 1M, sums 7 rows
```

---

## üéØ Window Function Cheat Sheet

```sql
-- Ranking
ROW_NUMBER() OVER (ORDER BY column)        -- 1, 2, 3, 4, 5
RANK() OVER (ORDER BY column)              -- 1, 2, 3, 3, 5
DENSE_RANK() OVER (ORDER BY column)        -- 1, 2, 3, 3, 4
NTILE(n) OVER (ORDER BY column)            -- Divide into n buckets

-- Aggregates
SUM(column) OVER (...)
AVG(column) OVER (...)
COUNT(*) OVER (...)
MIN(column) OVER (...)
MAX(column) OVER (...)

-- Access other rows
LAG(column, offset) OVER (ORDER BY ...)    -- Previous row
LEAD(column, offset) OVER (ORDER BY ...)   -- Next row
FIRST_VALUE(column) OVER (...)             -- First in window
LAST_VALUE(column) OVER (...)              -- Last in window
NTH_VALUE(column, n) OVER (...)            -- Nth in window

-- Partition by group
OVER (PARTITION BY department)

-- Order within partition
OVER (PARTITION BY department ORDER BY salary DESC)

-- Define window frame
ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
ROWS BETWEEN CURRENT ROW AND 3 FOLLOWING
ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
```

---

## üí™ Next Steps

**1. Replace Subqueries** (Today):
- Find queries with nested subqueries for rankings/aggregates
- Refactor using window functions
- Benchmark performance improvement

**2. Add Analytics Features** (This Week):
- Leaderboards with `RANK()`
- Running totals with `SUM() OVER (ORDER BY ...)`
- Moving averages with `ROWS BETWEEN`
- Day-over-day changes with `LAG()`

**3. Advanced Segmentation** (Next Week):
- Customer segmentation with `NTILE()`
- Streak detection with gap & islands
- Percentile calculations with `PERCENT_RANK()`

**Remember**: Window functions keep all rows. If you need one value per group, use GROUP BY instead.

---

## üîó Learn More

**Next POCs**:
- **POC #26: Table Partitioning** - Speed up window functions on large tables
- **POC #27: Foreign Keys** - Referential integrity
- **POC #28: Check Constraints** - Data validation rules

**Related Topics**:
- **POC #24: CTEs** - Combine with window functions for complex queries
- **POC #23: Materialized Views** - Cache window function results
- **POC #14: EXPLAIN Analysis** - Optimize window function performance
